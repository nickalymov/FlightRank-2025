# ‚úàÔ∏è FlightRank 2025: Aeroclub RecSys Cup (15th Place Solution)

This repository contains the code and notebooks for my solution to the [Kaggle FlightRank 2025: Aeroclub RecSys Cup](https://www.kaggle.com/competitions/aeroclub-recsys-2025) competition. My final submission achieved **15th place out of 578 participants**.

### üèÜ Competition Overview

The goal of the competition was to develop a recommendation system to rank flight options for business travelers. Given a set of available flights for a user's search query (`ranker_id`), the task was to predict the `selected` flight, formulating it as a ranking problem evaluated by NDCG.

---

### üìä Performance and Results

My final solution was an ensemble of several high-performing models. The graph below illustrates the progression of my Public and Private scores across 42 submissions, showing the impact of different experiments and model architectures.

<img width="4200" height="2400" alt="image" src="https://github.com/user-attachments/assets/a4a253cb-a085-4f5f-ab8d-baba22add423" />

---

### üìÇ Repository Structure

*   `data/`: Contains the original competition data provided by Kaggle.
*   `mydata/`: Contains pre-processed datasets generated by the cleaning notebooks.
    *   `mydata/1/`: The first version of cleaned data.
    *   `mydata/2/`: The second version, which includes additional features extracted from raw JSON files.
*   `notebooks/`: Contains all 42 Jupyter notebooks detailing the model development, feature engineering experiments, and ensembling strategies.

---

### üõ†Ô∏è Methodology and Model Evolution

My approach evolved significantly throughout the competition, starting with simple neural networks and progressing to complex feature engineering and ensembling.

**1. Initial Models: Deep Learning Foundations (Notebooks 1-12)**
*   **Simple MLP:** My first attempts involved a standard Multi-Layer Perceptron (MLP) using PyTorch to establish a baseline.
*   **Deep & Cross Network (DCN):** I quickly moved to a Deep & Cross Network (DCNv2) architecture. This model is well-suited for tabular data as it combines a standard deep network with a cross-network that explicitly learns feature interactions. This formed the core architecture for most of my experiments.
*   **Hyperparameter Tuning:** Early experiments involved tuning the number of epochs, MLP layer dimensions (e.g., `[1024, 512, 256]`), and learning rates.

**2. Feature Engineering (Notebooks 13-35)**
The most significant score improvements came from iterative feature engineering:
*   **Group-wise Statistical Features:** I calculated statistics (`min`, `max`, `mean`, `std`) for key numerical columns (`totalPrice`, `legs0_duration`) grouped by `ranker_id`. Features like `price_diff_from_min` and `price_norm_by_group` helped the model understand the relative quality of an option within a single search session.
*   **Geospatial Features:** Using the `airports.csv` dataset, I created lookup tables for airport coordinates. This allowed for the dynamic calculation of Haversine distances for each flight segment, providing a powerful signal.
*   **Convenience Features:** I engineered features to quantify the convenience of a flight, such as the number of layovers and whether there was a change of airline during a leg.

**3. Advanced Model Experiments**
*   **Pairwise Loss (Notebook 16):** I experimented with a pairwise approach using `MarginRankingLoss`. This method directly optimizes for the relative order of pairs (one selected, one not-selected flight). While a valuable experiment, it yielded a very low score, indicating issues with implementation or its suitability for this dataset compared to pointwise methods.
*   **Weighted Embeddings:** For handling unseen categorical values, I implemented weighted averaging of embeddings based on their frequency in the training data, providing a more robust representation for the "unknown" category.

**4. Ensembling and Final Submissions (Notebooks 38, 39, 41, 42)**
My best results were achieved by ensembling multiple models.
*   **Weighted Blending:** I used a weighted average of the ranks produced by my best-performing notebooks. The weights were based on the models' Public Leaderboard scores.
*   **Blending Public Notebooks:** My final submission (`submission_42.csv`), which scored **0.52293**, was a blend of my own models and two high-scoring public notebooks:
    *   [Getting start with CatBoost + Optuna](https://www.kaggle.com/code/roxhap/getting) by [ROXANNE KHAZIDINOVA](https://www.kaggle.com/roxhap)
    *   [FlightRank| N Folds | Holdout | Single Model](https://www.kaggle.com/code/letemoin/flightrank-n-folds-holdout-single-model) by [KONSTANTIN](https://www.kaggle.com/letemoin)

---

### üöÄ Analysis of Top Solutions & What Was Missing

By analyzing the write-ups from the top 3 teams, several key themes emerged that could have further improved my score.

**Key Strategies from Top Teams:**
1.  **Specialized Ranking Models:** The top teams heavily utilized Gradient Boosting Decision Tree (GBDT) models specifically designed for ranking, such as **XGBoost (`rank:pairwise`, `rank:ndcg`)** and **LightGBM (`lambdarank`)**. [1, 2, 3] My solution relied almost exclusively on a DCN architecture, which may not have been as effective as these specialized tree-based rankers.
2.  **Advanced Feature Engineering:**
    *   **Count-Based & Historical Features:** The 1st place solution created extensive count-based features, including those derived from a temporal split to prevent data leakage. They also engineered features based on `(ranker_id, flight_hash)` groups, which proved crucial. [1]
    *   **Cross-Validation Aware Features:** The 2nd place solution built behavioral features (e.g., user's historical price sensitivity) in a cross-validation-aware manner to avoid leakage. [3]
3.  **Robust Validation Strategy:**
    *   **Temporal Split:** Top solutions emphasized a strict time-based split for validation, using older data to train and newer data to validate, which better simulates a real-world production environment. [1]
    *   **GroupKFold:** `GroupKFold` was used to ensure that all flights from the same `ranker_id` were kept in the same fold, which is critical for ranking problems. [3]
4.  **Bucket-Wise Ensembling:** The 2nd place winner developed a sophisticated ensembling strategy where different model weights were optimized for different `ranker_id` group sizes (e.g., users seeing 15 options vs. 150). [3]

**What My Solution Lacked:**
*   **GBDT Rankers:** My primary weakness was not incorporating `LGBMRanker` or `XGBRanker`. These models are often state-of-the-art for ranking on tabular data and likely would have outperformed my DCN models.
*   **Sophisticated Validation:** I used a simple random shuffle for training batches, whereas a time-based or group-aware validation scheme would have provided more reliable local scoring and prevented overfitting.
*   **Deeper Feature Interactions:** While the DCN handles interactions implicitly, the top solutions engineered explicit interaction features (e.g., user preferences for certain airlines on specific routes) and historical counters, which captured more complex behavioral patterns.

---

### üîß How to Run

1.  **Setup:**
    *   Clone the repository.
    *   Install the required libraries: `pip install -r requirements.txt`.
2.  **Data:**
    *   Download the competition data from [Kaggle](https://www.kaggle.com/competitions/aeroclub-recsys-2025/data) and place the files in the `data/` directory.
    *   Place the raw JSON files (if available) in `data/raw/`.
3.  **Preprocessing:**
    *   Run the notebooks in `mydata/` (e.g., `1_clean_data.ipynb`) to generate the cleaned parquet files.
4.  **Training & Inference:**
    *   The notebooks in the `notebooks/` directory are numbered in the order of my experiments. You can run them sequentially to reproduce the submissions. The final ensembling notebooks (e.g., `42.ipynb`) combine the outputs of previous models.
