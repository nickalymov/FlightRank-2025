{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# | БЛОК 1: Импорт необходимых библиотек                                   |\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import gc "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T23:43:38.528892Z",
     "start_time": "2025-08-11T23:43:35.236149400Z"
    }
   },
   "id": "e56c094aa9561d42",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# | БЛОК 2: Конфигурация проекта                                           |\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "VER = 20\n",
    "CONFIG = {\n",
    "    'train_path': f'C:/Users/Николай/PycharmProjects/FlightRank_2025/mydata/1/1_train.parquet',\n",
    "    'test_path': f'C:/Users/Николай/PycharmProjects/FlightRank_2025/mydata/1/1_test.parquet',\n",
    "    'sample_submission_path': f'C:/Users/Николай/PycharmProjects/FlightRank_2025/data/sample_submission.parquet',\n",
    "\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu', \n",
    "    'SEED': 42,\n",
    "    'BATCH_SIZE': 8192*4, \n",
    "    'LR': 0.001, \n",
    "    'EPOCHS': 3, \n",
    "    'WEIGHT_DECAY': 1e-5, \n",
    "\n",
    "    'embedding_dims': {},\n",
    "    'dropout_rate': 0.1, \n",
    "    'mlp_dims': [1024, 512, 256], \n",
    "    'num_cross_layers': 4,\n",
    "    'output_dim': 1 \n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['SEED'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T23:43:39.224499100Z",
     "start_time": "2025-08-11T23:43:39.208278100Z"
    }
   },
   "id": "17317e521eab514c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# | БЛОК 3: Загрузка данных и определение размеров эмбеддингов             |\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "train_df = pd.read_parquet(CONFIG['train_path'], engine='pyarrow')\n",
    "test_df = pd.read_parquet(CONFIG['test_path'], engine='pyarrow')\n",
    "\n",
    "data_cols= [\n",
    "    'legs0_arrivalAt', 'legs0_departureAt', 'legs1_arrivalAt', 'legs1_departureAt', 'requestDate'\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    'legs0_duration', 'legs0_segments0_duration', 'legs0_segments1_duration', 'legs0_segments2_duration', 'legs0_segments3_duration', 'legs1_duration', 'legs1_segments0_duration', 'legs1_segments1_duration', 'legs1_segments2_duration', 'legs1_segments3_duration', 'miniRules0_monetaryAmount', 'miniRules1_monetaryAmount', 'taxes', 'totalPrice', \n",
    "]\n",
    "\n",
    "bool_cols = ['isAccess3D', 'isVip', 'sex']\n",
    "\n",
    "cat_cols = [col for col in train_df.columns if col not in data_cols and col not in num_cols and col not in bool_cols and col not in  ['ranker_id', 'selected', 'frequentFlyer']]\n",
    "\n",
    "frequentFlyer_col = 'frequentFlyer'\n",
    "\n",
    "ranker_id_col = 'ranker_id'\n",
    "\n",
    "selected_col = 'selected'\n",
    "\n",
    "\n",
    "for col in cat_cols:\n",
    "    num_unique_values = train_df[col].nunique() + 1\n",
    "    embedding_dim = int(np.sqrt(num_unique_values))\n",
    "    CONFIG['embedding_dims'][col] = (num_unique_values, embedding_dim)\n",
    "    \n",
    "\n",
    "# 1.1 Создание словаря для frequentFlyer и обновление конфига\n",
    "all_ff_codes = train_df[frequentFlyer_col].str.split('/').explode().dropna().unique()\n",
    "ff_code_to_idx = {code: i for i, code in enumerate(all_ff_codes)}\n",
    "ff_unknown_idx = len(ff_code_to_idx)\n",
    "ff_embedding_dim = int(np.sqrt(len(all_ff_codes) + 1))\n",
    "CONFIG['embedding_dims'][frequentFlyer_col] = (len(ff_code_to_idx) + 1, ff_embedding_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-11T23:44:09.980435200Z",
     "start_time": "2025-08-11T23:43:39.229498500Z"
    }
   },
   "id": "f2c9ccc9357858e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# | БЛОК 4: Определение модели (с новыми признаками-разницами)              |\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "class FlightRankModel(nn.Module):\n",
    "    def __init__(self, config, num_cols, cat_cols, bool_cols, data_cols, frequentFlyer_col):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.bool_cols = bool_cols\n",
    "        self.data_cols = data_cols\n",
    "        self.frequentFlyer_col = frequentFlyer_col\n",
    "\n",
    "        # --- 1. ВХОДНАЯ ЧАСТЬ ---\n",
    "        self.embedding_layers = nn.ModuleDict({\n",
    "            col: nn.Embedding(num_embeddings=dims[0], embedding_dim=dims[1])\n",
    "            for col, dims in config['embedding_dims'].items() if col != frequentFlyer_col\n",
    "        })\n",
    "        \n",
    "        ff_dims = config['embedding_dims'][frequentFlyer_col]\n",
    "        self.ff_embedding_layer = nn.Embedding(num_embeddings=ff_dims[0], embedding_dim=ff_dims[1])\n",
    "\n",
    "        cat_embedding_dim = sum(dims[1] for col, dims in config['embedding_dims'].items() if col != frequentFlyer_col)\n",
    "        ff_embedding_dim = ff_dims[1]\n",
    "        numerical_dim = len(self.num_cols)\n",
    "        boolean_dim = len(self.bool_cols)\n",
    "        cyclical_dim = len(self.data_cols) * 8\n",
    "        time_to_departure_dim = 1\n",
    "        \n",
    "        # ИЗМЕНЕНИЕ: Добавляем размер для новых признаков (3 diff-фичи на каждую числовую колонку)\n",
    "        diff_features_dim = len(self.num_cols) * 3\n",
    "        \n",
    "        self.input_dim = (cat_embedding_dim + ff_embedding_dim + numerical_dim + \n",
    "                          boolean_dim + cyclical_dim + time_to_departure_dim + diff_features_dim)\n",
    "        \n",
    "        # ИЗМЕНЕНИЕ: Обновляем размер BatchNorm\n",
    "        self.all_numerical_batch_norm = nn.BatchNorm1d(\n",
    "            numerical_dim + boolean_dim + cyclical_dim + time_to_departure_dim + diff_features_dim\n",
    "        )\n",
    "        \n",
    "        # --- 2. CROSS & DEEP сети (остаются без изменений, т.к. input_dim уже обновлен) ---\n",
    "        self.cross_net = nn.ModuleList([\n",
    "            nn.Linear(self.input_dim, self.input_dim) \n",
    "            for _ in range(config['num_cross_layers'])\n",
    "        ])\n",
    "\n",
    "        deep_layers = []\n",
    "        layer_dims = [self.input_dim] + config['mlp_dims']\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            deep_layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            deep_layers.append(nn.ReLU())\n",
    "        self.deep_net = nn.Sequential(*deep_layers)\n",
    "        \n",
    "        self.final_layer = nn.Linear(config['mlp_dims'][-1], config['output_dim'])\n",
    "\n",
    "    def forward(self, x_dict):\n",
    "        # --- 1. Формирование x_0 ---\n",
    "        embedded_features = [self.embedding_layers[col](x_dict[col]) for col in self.cat_cols]\n",
    "        \n",
    "        # ... (логика для frequentFlyer без изменений) ...\n",
    "        list_of_indices = x_dict[self.frequentFlyer_col]\n",
    "        avg_ff_embeddings_list = []\n",
    "        for indices in list_of_indices:\n",
    "            if not indices:\n",
    "                avg_embedding = torch.zeros(self.ff_embedding_layer.embedding_dim, device=self.config['DEVICE'])\n",
    "            else:\n",
    "                indices_tensor = torch.tensor(indices, dtype=torch.long, device=self.config['DEVICE'])\n",
    "                embeddings = self.ff_embedding_layer(indices_tensor)\n",
    "                avg_embedding = embeddings.mean(dim=0)\n",
    "            avg_ff_embeddings_list.append(avg_embedding)\n",
    "        avg_ff_embedding_batch = torch.stack(avg_ff_embeddings_list, dim=0)\n",
    "        embedded_features.append(avg_ff_embedding_batch)\n",
    "        concatenated_embeddings = torch.cat(embedded_features, dim=1)\n",
    "\n",
    "        # Собираем все числоподобные признаки\n",
    "        numerical_inputs = [x_dict['numerical'], x_dict['boolean']]\n",
    "        for col in self.data_cols:\n",
    "            date_tensor = x_dict[f'{col}_components']\n",
    "            numerical_inputs.append(torch.sin(2 * np.pi * date_tensor[:, 0] / 59.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.cos(2 * np.pi * date_tensor[:, 0] / 59.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.sin(2 * np.pi * date_tensor[:, 1] / 23.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.cos(2 * np.pi * date_tensor[:, 1] / 23.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.sin(2 * np.pi * date_tensor[:, 2] / 6.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.cos(2 * np.pi * date_tensor[:, 2] / 6.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.sin(2 * np.pi * date_tensor[:, 3] / 365.0).unsqueeze(1))\n",
    "            numerical_inputs.append(torch.cos(2 * np.pi * date_tensor[:, 3] / 365.0).unsqueeze(1))\n",
    "        \n",
    "        time_to_departure = (x_dict['legs0_departureAt_unix'] - x_dict['requestDate_unix']) / 60.0\n",
    "        numerical_inputs.append(time_to_departure.unsqueeze(1))\n",
    "        \n",
    "        # ИЗМЕНЕНИЕ: Добавляем новые признаки-разницы\n",
    "        numerical_inputs.append(x_dict['diff_features'])\n",
    "        \n",
    "        processed_numerical_all = torch.cat(numerical_inputs, dim=1)\n",
    "        processed_numerical_all = self.all_numerical_batch_norm(processed_numerical_all)\n",
    "\n",
    "        x_0 = torch.cat([concatenated_embeddings, processed_numerical_all], dim=1)\n",
    "        \n",
    "        # ... (Cross-сеть, Deep-сеть и final_layer без изменений) ...\n",
    "        x_cross = x_0\n",
    "        for layer in self.cross_net:\n",
    "            x_cross = x_0 * torch.sigmoid(layer(x_cross)) + x_cross\n",
    "        deep_output = self.deep_net(x_cross)\n",
    "        final_output = self.final_layer(deep_output)\n",
    "        \n",
    "        return final_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87a6b1b33057105",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начало предобработки для обучения ---\n",
      "Статистика Min-Max для нормализации рассчитана.\n",
      "Предварительный расчет групповых статистик...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Создание статистических фич: 100%|██████████| 14/14 [01:53<00:00,  8.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистические фичи добавлены в DataFrame'ы.\n",
      "\n",
      "--- Начало обучения на 3 эпох ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1/3: 100%|██████████| 554/554 [37:33<00:00,  4.07s/it, avg_loss=0.0257] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый средний лосс за эпоху 1: 0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 2/3: 100%|██████████| 554/554 [33:47<00:00,  3.66s/it, avg_loss=0.0211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый средний лосс за эпоху 2: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 3/3: 100%|██████████| 554/554 [33:41<00:00,  3.65s/it, avg_loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итоговый средний лосс за эпоху 3: 0.0199\n",
      "\n",
      "--- Обновление эмбеддингов для неизвестных категорий (взвешенное усреднение) ---\n",
      "Обработка колонки: companyID\n",
      "Обработка колонки: corporateTariffCode\n",
      "Обработка колонки: nationality\n",
      "Обработка колонки: legs0_segments0_aircraft_code\n",
      "Обработка колонки: legs0_segments0_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs0_segments0_arrivalTo_airport_iata\n",
      "Обработка колонки: legs0_segments0_baggageAllowance_quantity\n",
      "Обработка колонки: legs0_segments0_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs0_segments0_cabinClass\n",
      "Обработка колонки: legs0_segments0_departureFrom_airport_iata\n",
      "Обработка колонки: legs0_segments0_flightNumber\n",
      "Обработка колонки: legs0_segments0_marketingCarrier_code\n",
      "Обработка колонки: legs0_segments0_operatingCarrier_code\n",
      "Обработка колонки: legs0_segments0_seatsAvailable\n",
      "Обработка колонки: legs0_segments1_aircraft_code\n",
      "Обработка колонки: legs0_segments1_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs0_segments1_arrivalTo_airport_iata\n",
      "Обработка колонки: legs0_segments1_baggageAllowance_quantity\n",
      "Обработка колонки: legs0_segments1_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs0_segments1_cabinClass\n",
      "Обработка колонки: legs0_segments1_departureFrom_airport_iata\n",
      "Обработка колонки: legs0_segments1_flightNumber\n",
      "Обработка колонки: legs0_segments1_marketingCarrier_code\n",
      "Обработка колонки: legs0_segments1_operatingCarrier_code\n",
      "Обработка колонки: legs0_segments1_seatsAvailable\n",
      "Обработка колонки: legs0_segments2_aircraft_code\n",
      "Обработка колонки: legs0_segments2_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs0_segments2_arrivalTo_airport_iata\n",
      "Обработка колонки: legs0_segments2_baggageAllowance_quantity\n",
      "Обработка колонки: legs0_segments2_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs0_segments2_cabinClass\n",
      "Обработка колонки: legs0_segments2_departureFrom_airport_iata\n",
      "Обработка колонки: legs0_segments2_flightNumber\n",
      "Обработка колонки: legs0_segments2_marketingCarrier_code\n",
      "Обработка колонки: legs0_segments2_operatingCarrier_code\n",
      "Обработка колонки: legs0_segments2_seatsAvailable\n",
      "Обработка колонки: legs0_segments3_aircraft_code\n",
      "Обработка колонки: legs0_segments3_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs0_segments3_arrivalTo_airport_iata\n",
      "Обработка колонки: legs0_segments3_baggageAllowance_quantity\n",
      "Обработка колонки: legs0_segments3_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs0_segments3_cabinClass\n",
      "Обработка колонки: legs0_segments3_departureFrom_airport_iata\n",
      "Обработка колонки: legs0_segments3_flightNumber\n",
      "Обработка колонки: legs0_segments3_marketingCarrier_code\n",
      "Обработка колонки: legs0_segments3_operatingCarrier_code\n",
      "Обработка колонки: legs0_segments3_seatsAvailable\n",
      "Обработка колонки: legs1_segments0_aircraft_code\n",
      "Обработка колонки: legs1_segments0_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs1_segments0_arrivalTo_airport_iata\n",
      "Обработка колонки: legs1_segments0_baggageAllowance_quantity\n",
      "Обработка колонки: legs1_segments0_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs1_segments0_cabinClass\n",
      "Обработка колонки: legs1_segments0_departureFrom_airport_iata\n",
      "Обработка колонки: legs1_segments0_flightNumber\n",
      "Обработка колонки: legs1_segments0_marketingCarrier_code\n",
      "Обработка колонки: legs1_segments0_operatingCarrier_code\n",
      "Обработка колонки: legs1_segments0_seatsAvailable\n",
      "Обработка колонки: legs1_segments1_aircraft_code\n",
      "Обработка колонки: legs1_segments1_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs1_segments1_arrivalTo_airport_iata\n",
      "Обработка колонки: legs1_segments1_baggageAllowance_quantity\n",
      "Обработка колонки: legs1_segments1_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs1_segments1_cabinClass\n",
      "Обработка колонки: legs1_segments1_departureFrom_airport_iata\n",
      "Обработка колонки: legs1_segments1_flightNumber\n",
      "Обработка колонки: legs1_segments1_marketingCarrier_code\n",
      "Обработка колонки: legs1_segments1_operatingCarrier_code\n",
      "Обработка колонки: legs1_segments1_seatsAvailable\n",
      "Обработка колонки: legs1_segments2_aircraft_code\n",
      "Обработка колонки: legs1_segments2_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs1_segments2_arrivalTo_airport_iata\n",
      "Обработка колонки: legs1_segments2_baggageAllowance_quantity\n",
      "Обработка колонки: legs1_segments2_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs1_segments2_cabinClass\n",
      "Обработка колонки: legs1_segments2_departureFrom_airport_iata\n",
      "Обработка колонки: legs1_segments2_flightNumber\n",
      "Обработка колонки: legs1_segments2_marketingCarrier_code\n",
      "Обработка колонки: legs1_segments2_operatingCarrier_code\n",
      "Обработка колонки: legs1_segments2_seatsAvailable\n",
      "Обработка колонки: legs1_segments3_aircraft_code\n",
      "Обработка колонки: legs1_segments3_arrivalTo_airport_city_iata\n",
      "Обработка колонки: legs1_segments3_arrivalTo_airport_iata\n",
      "Обработка колонки: legs1_segments3_baggageAllowance_quantity\n",
      "Обработка колонки: legs1_segments3_baggageAllowance_weightMeasurementType\n",
      "Обработка колонки: legs1_segments3_cabinClass\n",
      "Обработка колонки: legs1_segments3_departureFrom_airport_iata\n",
      "Обработка колонки: legs1_segments3_flightNumber\n",
      "Обработка колонки: legs1_segments3_marketingCarrier_code\n",
      "Обработка колонки: legs1_segments3_operatingCarrier_code\n",
      "Обработка колонки: legs1_segments3_seatsAvailable\n",
      "Обработка колонки: miniRules0_percentage\n",
      "Обработка колонки: miniRules0_statusInfos\n",
      "Обработка колонки: miniRules1_percentage\n",
      "Обработка колонки: miniRules1_statusInfos\n",
      "Обработка колонки: pricingInfo_isAccessTP\n",
      "Обработка колонки: profileId\n",
      "Обработка колонки: departure_origin\n",
      "Обработка колонки: departure_destination\n",
      "Обработка колонки: return_origin\n",
      "Обработка колонки: return_destination\n",
      "Обработка колонки: frequentFlyer\n",
      "\n",
      "--- Генерация предсказаний для теста ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Предсказание: 100%|██████████| 211/211 [06:07<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# | БЛОК 5: Предобработка, обучение и предсказание (с новыми фичами)        |\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. Подготовительный этап ---\n",
    "print(\"--- Начало предобработки для обучения ---\")\n",
    "\n",
    "# 1.1 Расчет Min/Max для нормализации\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "num_min = torch.tensor(combined_df[num_cols].astype(np.float32).min().values, dtype=torch.float32)\n",
    "num_max = torch.tensor(combined_df[num_cols].astype(np.float32).max().values, dtype=torch.float32)\n",
    "num_range = num_max - num_min\n",
    "num_range[num_range == 0] = 1e-9\n",
    "print(\"Статистика Min-Max для нормализации рассчитана.\")\n",
    "\n",
    "# 1.2 ИЗМЕНЕНИЕ: Предварительный расчет групповых статистик\n",
    "print(\"Предварительный расчет групповых статистик...\")\n",
    "stat_cols_mean = []\n",
    "stat_cols_max = []\n",
    "stat_cols_min = []\n",
    "\n",
    "for col in tqdm(num_cols, desc=\"Создание статистических фич\"):\n",
    "    mean_col_name = f'{col}_mean_by_ranker'\n",
    "    max_col_name = f'{col}_max_by_ranker'\n",
    "    min_col_name = f'{col}_min_by_ranker'\n",
    "    \n",
    "    combined_df[mean_col_name] = combined_df.groupby('ranker_id')[col].transform('mean')\n",
    "    combined_df[max_col_name] = combined_df.groupby('ranker_id')[col].transform('max')\n",
    "    combined_df[min_col_name] = combined_df.groupby('ranker_id')[col].transform('min')\n",
    "    \n",
    "    stat_cols_mean.append(mean_col_name)\n",
    "    stat_cols_max.append(max_col_name)\n",
    "    stat_cols_min.append(min_col_name)\n",
    "\n",
    "# Разделяем обратно на train и test\n",
    "train_df = combined_df.iloc[:len(train_df)].copy()\n",
    "test_df = combined_df.iloc[len(train_df):].copy()\n",
    "del combined_df; gc.collect()\n",
    "print(\"Статистические фичи добавлены в DataFrame'ы.\")\n",
    "\n",
    "\n",
    "# --- 2. Функция для подготовки батчей ---\n",
    "def get_batch(df, indices, device):\n",
    "    batch_df = df.iloc[indices]\n",
    "    x = {}\n",
    "\n",
    "    for col in cat_cols:\n",
    "        x[col] = torch.tensor(batch_df[col].values.astype(np.int64), dtype=torch.long, device=device)\n",
    "    \n",
    "    # Нормализуем оригинальные числовые колонки\n",
    "    numerical_tensor = torch.tensor(batch_df[num_cols].values.astype(np.float32), device=device)\n",
    "    normalized_numerical = (numerical_tensor - num_min.to(device)) / num_range.to(device)\n",
    "    x['numerical'] = normalized_numerical\n",
    "    \n",
    "    x['boolean'] = torch.tensor(batch_df[bool_cols].values.astype(np.float32), device=device)\n",
    "    \n",
    "    for col in data_cols:\n",
    "        # ... (обработка дат без изменений) ...\n",
    "        dt_series = pd.to_datetime(batch_df[col], errors='coerce')\n",
    "        x[f'{col}_components'] = torch.tensor(np.vstack([dt_series.dt.minute.fillna(0), dt_series.dt.hour.fillna(0), dt_series.dt.dayofweek.fillna(0), dt_series.dt.dayofyear.fillna(0)]).T, dtype=torch.float32, device=device)\n",
    "        x[f'{col}_unix'] = torch.tensor(dt_series.astype(np.int64).values // 10**9, dtype=torch.float32, device=device)\n",
    "        \n",
    "    # ИЗМЕНЕНИЕ: Вычисляем признаки-разницы\n",
    "    # Нормализуем групповые статистики, используя те же min/max, что и для оригинальных колонок\n",
    "    stats_mean_tensor = (torch.tensor(batch_df[stat_cols_mean].values.astype(np.float32), device=device) - num_min.to(device)) / num_range.to(device)\n",
    "    stats_max_tensor = (torch.tensor(batch_df[stat_cols_max].values.astype(np.float32), device=device) - num_min.to(device)) / num_range.to(device)\n",
    "    stats_min_tensor = (torch.tensor(batch_df[stat_cols_min].values.astype(np.float32), device=device) - num_min.to(device)) / num_range.to(device)\n",
    "    \n",
    "    # Вычисляем разницы\n",
    "    diff_mean = normalized_numerical - stats_mean_tensor\n",
    "    diff_max = normalized_numerical - stats_max_tensor\n",
    "    diff_min = normalized_numerical - stats_min_tensor\n",
    "    \n",
    "    # Объединяем все разницы в один тензор\n",
    "    x['diff_features'] = torch.cat([diff_mean, diff_max, diff_min], dim=1)\n",
    "    \n",
    "    # ... (обработка frequentFlyer и y без изменений) ...\n",
    "    ff_str_list = batch_df[frequentFlyer_col].fillna('').tolist()\n",
    "    list_of_indices = [[ff_code_to_idx.get(code, ff_unknown_idx) for code in s.split('/') if code] for s in ff_str_list]\n",
    "    x[frequentFlyer_col] = list_of_indices\n",
    "    y = None\n",
    "    if 'selected' in batch_df.columns:\n",
    "        y = torch.tensor(batch_df['selected'].values.astype(float), dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "# --- 3. Инициализация и цикл обучения ---\n",
    "model = FlightRankModel(CONFIG, num_cols, cat_cols, bool_cols, data_cols, frequentFlyer_col).to(CONFIG['DEVICE'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=CONFIG['LR'])\n",
    "\n",
    "print(f\"\\n--- Начало обучения на {CONFIG['EPOCHS']} эпох ---\")\n",
    "for epoch in range(CONFIG['EPOCHS']):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    shuffled_indices = np.random.permutation(len(train_df))\n",
    "    num_batches = (len(train_df) + CONFIG['BATCH_SIZE'] - 1) // CONFIG['BATCH_SIZE']\n",
    "   \n",
    "    progress_bar = tqdm(range(num_batches), desc=f\"Эпоха {epoch + 1}/{CONFIG['EPOCHS']}\")\n",
    "    \n",
    "    for i in progress_bar:\n",
    "        batch_indices = shuffled_indices[i * CONFIG['BATCH_SIZE'] : (i + 1) * CONFIG['BATCH_SIZE']]\n",
    "        x_batch, y_batch = get_batch(train_df, batch_indices, CONFIG['DEVICE'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        current_avg_loss = running_loss / (i + 1)\n",
    "        progress_bar.set_postfix(avg_loss=f'{current_avg_loss:.4f}')\n",
    "        \n",
    "    print(f\"Итоговый средний лосс за эпоху {epoch + 1}: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "# --- 4. Обработка эмбеддингов для неизвестных категорий ---\n",
    "print(\"\\n--- Обновление эмбеддингов для неизвестных категорий (взвешенное усреднение) ---\")\n",
    "with torch.no_grad():\n",
    "    for col, layer in model.embedding_layers.items():\n",
    "        if col not in CONFIG['embedding_dims']:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Обработка колонки: {col}\")\n",
    "        counts = train_df[col].value_counts()\n",
    "        \n",
    "        known_indices = torch.tensor(counts.index.values, dtype=torch.long, device=CONFIG['DEVICE'])\n",
    "        weights = torch.tensor(counts.values, dtype=torch.float32, device=CONFIG['DEVICE'])\n",
    "        \n",
    "        known_embeddings = layer.weight.data[known_indices]\n",
    "        \n",
    "        weighted_sum_of_vectors = (known_embeddings * weights.unsqueeze(1)).sum(dim=0)\n",
    "        \n",
    "        total_weight = weights.sum()\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            mean_embedding = weighted_sum_of_vectors / total_weight\n",
    "            unknown_idx = CONFIG['embedding_dims'][col][0] - 1\n",
    "            layer.weight.data[unknown_idx] = mean_embedding\n",
    "\n",
    "    print(\"Обработка колонки: frequentFlyer\")\n",
    "    ff_counts = train_df[frequentFlyer_col].str.split('/').explode().dropna().value_counts()\n",
    "    \n",
    "    ff_known_indices_list = []\n",
    "    ff_weights_list = []\n",
    "    for code_str, count in ff_counts.items():\n",
    "        if code_str in ff_code_to_idx:\n",
    "            ff_known_indices_list.append(ff_code_to_idx[code_str])\n",
    "            ff_weights_list.append(count)\n",
    "\n",
    "    if ff_known_indices_list:\n",
    "        ff_known_indices = torch.tensor(ff_known_indices_list, dtype=torch.long, device=CONFIG['DEVICE'])\n",
    "        ff_weights = torch.tensor(ff_weights_list, dtype=torch.float32, device=CONFIG['DEVICE'])\n",
    "        \n",
    "        ff_known_embeddings = model.ff_embedding_layer.weight.data[ff_known_indices]\n",
    "        \n",
    "        weighted_sum_ff = (ff_known_embeddings * ff_weights.unsqueeze(1)).sum(dim=0)\n",
    "        total_weight_ff = ff_weights.sum()\n",
    "\n",
    "        if total_weight_ff > 0:\n",
    "            mean_ff_embedding = weighted_sum_ff / total_weight_ff\n",
    "            model.ff_embedding_layer.weight.data[ff_unknown_idx] = mean_ff_embedding\n",
    "\n",
    "# --- 5. Цикл предсказания ---\n",
    "print(\"\\n--- Генерация предсказаний для теста ---\")\n",
    "model.eval()\n",
    "test_preds = []\n",
    "num_test_batches = (len(test_df) + CONFIG['BATCH_SIZE'] - 1) // CONFIG['BATCH_SIZE']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_test_batches), desc=\"Предсказание\"):\n",
    "        test_indices = list(range(i * CONFIG['BATCH_SIZE'], min((i + 1) * CONFIG['BATCH_SIZE'], len(test_df))))\n",
    "        x_batch, _ = get_batch(test_df, test_indices, CONFIG['DEVICE'])\n",
    "        \n",
    "        outputs = model(x_batch)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "        test_preds.extend(preds)\n",
    "\n",
    "test_df['score'] = test_preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T01:38:56.423402900Z",
     "start_time": "2025-08-11T23:44:10.025625100Z"
    }
   },
   "id": "98c274af62d450e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Формирование файла для отправки ---\n",
      "\n",
      "Готово! Файл для отправки сохранен в: C:/Users/Николай/PycharmProjects/FlightRank_2025/submissions/submission_20.csv\n",
      "Пример содержимого submission файла:\n",
      "         Id                         ranker_id  selected\n",
      "0  18144679  c9373e5f772e43d593dd6ad2fa90f67a        26\n",
      "1  18144680  c9373e5f772e43d593dd6ad2fa90f67a        53\n",
      "2  18144681  c9373e5f772e43d593dd6ad2fa90f67a       224\n",
      "3  18144682  c9373e5f772e43d593dd6ad2fa90f67a        80\n",
      "4  18144683  c9373e5f772e43d593dd6ad2fa90f67a        97\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# | БЛОК 6: Формирование файла для отправки (submission)                   |\n",
    "# --------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Формирование файла для отправки ---\")\n",
    "\n",
    "CONFIG['submission_path'] = f'C:/Users/Николай/PycharmProjects/FlightRank_2025/submissions/submission_{VER}.csv'\n",
    "\n",
    "sample_submission_df = pd.read_parquet(CONFIG['sample_submission_path'])\n",
    "test_df['Id'] = sample_submission_df['Id'].values\n",
    "\n",
    "test_df['selected'] = test_df.groupby('ranker_id')['score'].rank(method='first', ascending=False).astype(int)\n",
    "\n",
    "submission_df = test_df[['Id', 'ranker_id', 'selected']]\n",
    "\n",
    "submission_df = submission_df.set_index('Id').loc[sample_submission_df['Id']].reset_index()\n",
    "\n",
    "submission_df.to_csv(CONFIG['submission_path'], index=False)\n",
    "\n",
    "print(f\"\\nГотово! Файл для отправки сохранен в: {CONFIG['submission_path']}\")\n",
    "print(\"Пример содержимого submission файла:\")\n",
    "print(submission_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T01:39:08.737590100Z",
     "start_time": "2025-08-12T01:38:56.431218600Z"
    }
   },
   "id": "2ed7a9e0b95ae013",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "18145372    0.018461\n18145373    0.009058\n18145374    0.000156\n18145375    0.003248\n18145376    0.002061\n              ...   \n25043143    0.015423\n25043144    0.230324\n25043145    0.016983\n25043146    0.272953\n25043147    0.006084\nName: score, Length: 6897776, dtype: float32"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['score']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T03:11:43.306079300Z",
     "start_time": "2025-08-12T03:11:43.195264200Z"
    }
   },
   "id": "a12b9b73040a2cd",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
